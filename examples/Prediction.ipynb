{"cells":[{"cell_type":"markdown","metadata":{"id":"YKjVXkOq5RvM"},"source":["# Prediction"]},{"cell_type":"markdown","source":["[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mixtape-Sessions/Machine-Learning/blob/main/Labs/Prediction.ipynb)"],"metadata":{"id":"fop3YacnNZFe"}},{"cell_type":"markdown","source":["Outcome to be predicted: $Y_i$\n","> *example:* a worker's log wage\n","\n","Characteristics (aka **features**): $X_i=\\left(X_{1i},\\ldots,X_{pi}\\right)'$\n","> *example:* education, age, state of birth, parents' education, cognitive ability, family background\n"],"metadata":{"id":"KhnbzPkYx-k9"}},{"cell_type":"code","source":["%matplotlib inline\n","\n","# import some useful packages\n","import pandas as pd\n","import numpy as np\n","from sklearn import linear_model\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import GridSearchCV\n","import warnings\n","warnings.filterwarnings('ignore')\n","from sklearn.model_selection import train_test_split\n","\n","\n","plt.style.use('seaborn-whitegrid')\n","\n","# read in data\n","nlsy=pd.read_csv('https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/nlsy97.csv?raw=true')\n","nlsy"],"metadata":{"id":"7IIIROqHoI4g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Least squares benchmark"],"metadata":{"id":"wizaAK2iRf95"}},{"cell_type":"code","source":["# generate dictionary of transformations of education\n","powerlist=[nlsy['educ']**j for j in np.arange(1,13)]\n","X=pd.concat(powerlist,axis=1)\n","X.columns = ['educ'+str(j) for j in np.arange(1,13)]\n","# standardize our X matrix (doesn't matter for OLS, but will matter for lasso below)\n","scaler = StandardScaler()\n","scaler.fit(X)\n","X_scaled = scaler.transform(X)\n","\n","# run least squares regression\n","# instantiate and fit our regression object:\n","reg=linear_model.LinearRegression().fit(X_scaled,nlsy['lnw_2016'])\n","# generate predicted values\n","yhat=reg.predict(X_scaled)\n"],"metadata":{"id":"25-Dc3CdTQ-s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"82SAx5UTaKQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot predicted values\n","lnwbar=nlsy.groupby('educ')['lnw_2016'].mean()\n","Xbar=pd.DataFrame({'educ':lnwbar.index.values})\n","powerlist=[Xbar['educ']**j for j in np.arange(1,13)]\n","Xbar=pd.concat(powerlist,axis=1)\n","Xbar.columns = X.columns\n","Xbar_scaled = scaler.transform(Xbar)\n","ybarhat=reg.predict(Xbar_scaled)\n","fig = plt.figure()\n","ax = plt.axes()\n","ax.plot(Xbar['educ1'],lnwbar,'bo',Xbar['educ1'],ybarhat,'g-');\n","plt.title(\"ln Wages by Education in the NLSY\")\n","plt.xlabel(\"years of schooling\")\n","plt.ylabel(\"ln wages\");"],"metadata":{"id":"WwPztFomD_I_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On your own: repeat the above but for a much more flexible function of education"],"metadata":{"id":"IONe9jQdThRM"}},{"cell_type":"code","source":[],"metadata":{"id":"OgAYhaEMTgEj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Use train_test_split to estimate out-of-sample prediction error\n","X_scaled_train,X_scaled_test,y_train,y_test = train_test_split(X_scaled,nlsy['lnw_2016'],random_state=42)"],"metadata":{"id":"LHL0qUs8f3EU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# regress log earnings on education (and powers) in training set only\n","reg12=linear_model.LinearRegression().fit(X_scaled_train,y_train)\n","reg12.score(X_scaled_test,y_test)"],"metadata":{"id":"a6m_N1wEgakP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see, least squares linear regression can approximate any continuous function and can certainly be used for prediction. Include a rich enough set of transformations, and OLS predictions will yield unbiased estimates of the true ideal predictor, the conditional expectation function. But these estimates will be quite noisy. Penalized regression can greatly reduce the variance, at the expense of some bias. But if the variance reduction is great enough, the predictions can have lower MSE. Back to the whiteboard!\n"],"metadata":{"id":"mtllIvMQ1V8X"}},{"cell_type":"markdown","source":["## Lasso in action"],"metadata":{"id":"eJZrKXiFRowv"}},{"cell_type":"markdown","source":["Welcome back! Let's see lasso in action:"],"metadata":{"id":"3zrJPzaPCkrm"}},{"cell_type":"code","source":["# fit lasso with a couple of different alphas and plot results\n","# instantiate and fit our lasso object\n","lasso1 = linear_model.Lasso(alpha=.001,max_iter=1000).fit(X_scaled,nlsy['lnw_2016'])\n","#generate predicted values\n","ybarhat1=lasso1.predict(Xbar_scaled)\n","\n","# same thing but with a different alpha\n","lasso2 = linear_model.Lasso(alpha=.01,max_iter=1000).fit(X_scaled,nlsy['lnw_2016'])\n","ybarhat2=lasso2.predict(Xbar_scaled)\n"],"metadata":{"id":"WpeyLwK8iV6k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot results"],"metadata":{"id":"FcrrwylQx0FI"}},{"cell_type":"code","source":["#@title\n","fig1,(ax11,ax12,ax13) = plt.subplots(1,3,figsize=(12, 4))\n","ax11.barh(Xbar.columns,reg.coef_,align='center');\n","ax11.set_title(\"OLS coefficients\")\n","ax11.set_xlabel(\"coefficient\")\n","ax12.barh(Xbar.columns,lasso1.coef_,align='center');\n","ax12.set_title(\"Lasso coefficients (alpha = {:.3f})\".format(lasso1.get_params()['alpha']))\n","ax12.set_xlabel(\"coefficient\")\n","ax13.barh(Xbar.columns,lasso2.coef_,align='center');\n","ax13.set_title(\"Lasso coefficients (alpha = {:.2f})\".format(lasso2.get_params()['alpha']))\n","ax13.set_xlabel(\"coefficient\")\n","fig2,(ax21,ax22,ax23) = plt.subplots(1,3,figsize=(12,4))\n","ax21.plot(Xbar['educ1'],lnwbar,'bo',Xbar['educ1'],ybarhat,'g-');\n","ax21.set_title(\"ln Wages by Education in the NLSY\")\n","ax21.set_xlabel(\"years of schooling\")\n","ax21.set_ylabel(\"ln wages\");\n","ax22.plot(Xbar['educ1'],lnwbar,'bo',Xbar['educ1'],ybarhat1,'g-');\n","ax22.set_title(\"ln Wages by Education in the NLSY\")\n","ax22.set_xlabel(\"years of schooling\")\n","ax22.set_ylabel(\"ln wages\");\n","ax23.plot(Xbar['educ1'],lnwbar,'bo',Xbar['educ1'],ybarhat2,'g-');\n","ax23.set_title(\"ln Wages by Education in the NLSY\")\n","ax23.set_xlabel(\"years of schooling\")\n","ax23.set_ylabel(\"ln wages\");"],"metadata":{"cellView":"form","id":"-Vk_qSIzxwsi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Play around with different values for alpha to see how the fit changes!"],"metadata":{"id":"osbAl2I0znmx"}},{"cell_type":"markdown","source":["### Data-driven tuning parameters: Cross-validation"],"metadata":{"id":"WLGCHAGPnce4"}},{"cell_type":"markdown","source":["Quick trip back to the whiteboard!"],"metadata":{"id":"HSbFFZLsuo9D"}},{"cell_type":"code","source":["# define grid for alpha\n","alpha_grid = {'alpha': [.0001,.001,.002, .004, .006, .008, .01, .012, .014, .016 ,.018, .02 ],'max_iter': [100000]}\n","# instantiate and fit our gridsearchcv object\n","grid_search = GridSearchCV(linear_model.Lasso(),alpha_grid,cv=5,return_train_score=True).fit(X_scaled,nlsy['lnw_2016'])\n","# print out the chosen value for alpha\n","print(\"Best alpha: \",grid_search.best_estimator_.get_params()['alpha'])"],"metadata":{"id":"YVaN9tFwnyMv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Lasso-guided variable selection\n","For illustrative purposes we've been using lasso to determine the functional form for a single underlying regressor: education. But lasso's real power comes in selecting among a large number of regressors."],"metadata":{"id":"Q7r54SvPDz6P"}},{"cell_type":"code","source":["# Define \"menu\" of regressors:\n","X=nlsy.drop(columns=['lnw_2016','exp'])\n","\n","# Divide into training and test set so we can honestly gauge predictive accuracy\n","X_train, X_test, y_train, y_test = train_test_split(X, nlsy['lnw_2016'],random_state=42)\n","# Scale regressors\n","scaler.fit(X_train)\n","X_train_scaled = scaler.transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","# Do cross-validated Lasso (the easy way!)\n","# instantiate and fit our lassocv object\n","lassocv=linear_model.LassoCV(random_state=42).fit(X_train_scaled,y_train)\n","# print out the chosen value for alpha\n","print(\"Chosen alpha: {:.3f}\".format(lassocv.alpha_))\n","# print the original number of regressors and the number selected by lasso\n","print(\"Number of regressors in the menu: \",len(X.columns))\n","print(\"Number of regressors selected by lasso: \",sum(lassocv.coef_!=0))\n","# print out accuracy on training and test test\n","print(\"Accuracy on training set: {:.3f}\".format(lassocv.score(X_train_scaled,y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(lassocv.score(X_test_scaled,y_test)))\n","# look at the coefficients\n","results = pd.DataFrame({'feature': X.columns[lassocv.coef_!=0],'coefficient': lassocv.coef_[lassocv.coef_!=0]})\n","results"],"metadata":{"id":"gZyPjQ6XIIuD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To try on your own: load the Oregon HIE data from earlier and try lassoing the OLS regression we did there. What do you notice?\n"],"metadata":{"id":"YDMH-869NLKF"}},{"cell_type":"code","source":["# Load Oregon HIE data . . ."],"metadata":{"id":"eczzwQy0R9Cu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ridge regression\n","First, whiteboard. Ridge is another flavor of penalized regression, like lasso. But unlike lasso, ridge penalizes the squares (not the absolute values) of the coefficients. As a result, ridge shrinks coefficients toward zero, but not all the way. Let's give it a try."],"metadata":{"id":"88wfVz0KozvT"}},{"cell_type":"code","source":["# Try it on your own first with the NLSY data! (Hint: instead of LassoCV, use RidgeCV(cv=5), and it doesn't let you set random_state.\n","# Also, don't forget to define your X and y variables if you re-defined them with the Oregon HIE data)\n","\n","# instantiate and fit your ridge object\n","\n","# print accuracy on training set and test set\n","\n","# inspect the coefficients\n"],"metadata":{"id":"Idti1cNTpG28"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cheat"],"metadata":{"id":"XcZtQsXipf9J"}},{"cell_type":"code","source":["ridgecv=linear_model.RidgeCV(cv=5,alphas=(.1,1,10,50,100,1000)).fit(X_train_scaled,y_train)\n","print(\"Chosen alpha: {:.3f}\".format(ridgecv.alpha_))\n","print(\"Accuracy on training set: {:.3f}\".format(ridgecv.score(X_train_scaled,y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(ridgecv.score(X_test_scaled,y_test)))\n","# look at the coefficients\n","results = pd.DataFrame({'feature': X.columns[ridgecv.coef_!=0],'coefficient': ridgecv.coef_[ridgecv.coef_!=0]})\n","results"],"metadata":{"id":"0r517ruPpjOb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ...\n","What do we learn about the relative performance of Lasso and Ridge in this setting? What could be the explanation?"],"metadata":{"id":"2rmCnmqRrTNt"}},{"cell_type":"markdown","source":["One way to compare Lasso and Ridge, is to visualize their coefficients:"],"metadata":{"id":"bQkGESRIrclS"}},{"cell_type":"code","source":["plt.plot(ridgecv.coef_, 's', label=\"RidgeCV\")\n","plt.plot(lassocv.coef_, 'o', label=\"LassoCV\")\n","plt.xlabel(\"Coefficient index\")\n","plt.ylabel(\"Coefficient magnitude\")\n","xlims = plt.xlim()\n","plt.hlines(0, xlims[0], xlims[1])\n","plt.xlim(xlims)\n","plt.legend()"],"metadata":{"id":"xZdMrWlvr2WK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Elastic Net: best of both worlds?\n","Elastic net combines lasso and ridge penalization. First, a bit of whiteboard, then let's give it a try."],"metadata":{"id":"AImQVY7ws12d"}},{"cell_type":"code","source":["# Try it yourself first!\n","\n","# instantiate and fit your elasticnetcv object (hint: it's linear_model.ElasticNetCV,\n","# and supply it a grid for the l1 ratio: l1_ratio=[.1, .5, .7, .9, .95, .99, 1]\n","\n","# print out the chosen l1 ratio and alpha\n","\n","# print out the original and selected number of regressors\n","\n","# print out accuracy in the training and test set\n","\n","# look at the coefficients\n","\n","\n"],"metadata":{"id":"xJUVDML4tBmK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cheat"],"metadata":{"id":"qF34B5ittE_x"}},{"cell_type":"code","source":["encv=linear_model.ElasticNetCV(random_state=42,l1_ratio=[.1, .5, .7, .9, .95, .99, 1]).fit(X_train_scaled,y_train)\n","print(\"Chosen l1 ratio: {:.3f}\".format(encv.l1_ratio_))\n","print(\"Chosen alpha: {:.3f}\".format(encv.alpha_))\n","print(\"Number of regressors in the menu: \",len(X.columns))\n","print(\"Number of regressors selected by elastic net: \",sum(encv.coef_!=0))\n","print(\"Accuracy on training set: {:.3f}\".format(encv.score(X_train_scaled,y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(encv.score(X_test_scaled,y_test)))\n","# look at the coefficients\n","results = pd.DataFrame({'feature': X.columns[encv.coef_!=0],'coefficient': encv.coef_[encv.coef_!=0]})\n","results"],"metadata":{"id":"hw1ZDHBTtGYx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ...\n","Not surprisingly, it doesn't look terribly different from lasso."],"metadata":{"id":"8uff-P-3v5aX"}},{"cell_type":"markdown","source":["## Decision Trees and Random Forests\n","First, a trip to the whiteboard"],"metadata":{"id":"FDNSEcQYwfhC"}},{"cell_type":"markdown","source":["Import some utilities:"],"metadata":{"id":"0bAOPFex3mW3"}},{"cell_type":"code","source":["#@title\n","import requests\n","url1 = 'https://www.dropbox.com/s/jgml061manxpawo/plot_2d_separator.py?raw=true'\n","url2 = 'https://www.dropbox.com/s/hlrrlwm4kt36awb/plot_interactive_tree.py?raw=true'\n","url3 = 'https://www.dropbox.com/s/e2cy203sr30a59z/plot_helpers.py?raw=true'\n","url4 = 'https://www.dropbox.com/s/aik5sgcwgz4brwn/tools.py?raw=true'\n","r1 = requests.get(url1)\n","r2 = requests.get(url2)\n","r3 = requests.get(url3)\n","r4 = requests.get(url4)\n","\n","# make sure your filename is the same as how you want to import\n","with open('plot_2d_separator.py', 'w') as f1:\n","    f1.write(r1.text)\n","\n","with open('plot_interactive_tree.py', 'w') as f2:\n","    f2.write(r2.text)\n","\n","with open('plot_helpers.py', 'w') as f3:\n","    f3.write(r3.text)\n","\n","with open('tools.py', 'w') as f4:\n","    f4.write(r4.text)\n","\n","# now we can import\n","import plot_helpers\n","import tools\n","import plot_2d_separator\n","import plot_interactive_tree"],"metadata":{"id":"1Pf7NHq_yYr7","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's illustrate how random forests average over a collection of individual trees:"],"metadata":{"id":"xkcy2fSU3s4f"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import export_graphviz\n","import graphviz\n","from sklearn.datasets import make_moons\n","\n","Xfake, yfake = make_moons(n_samples=100, noise=0.25, random_state=3)\n","Xfake_train, Xfake_test, yfake_train, yfake_test = train_test_split(Xfake, yfake, stratify=yfake,\n","                                                    random_state=42)\n","\n","#First a simple tree:\n","tree = DecisionTreeClassifier(max_depth=3).fit(Xfake_train,yfake_train)\n","fig1,ax = plt.subplots(1,1,figsize=(12, 8))\n","plot_interactive_tree.plot_tree_partition(Xfake_train, yfake_train, tree, ax=ax)\n","dot_data= export_graphviz(tree, out_file=None, impurity=False, filled=True)\n","graph = graphviz.Source(dot_data)\n","graph"],"metadata":{"id":"7e2Jy9y8agDw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We got trees down. Now to the whiteboard to talk about random forests"],"metadata":{"id":"oDep9IE5vwul"}},{"cell_type":"code","source":["# Now average over several trees:\n","\n","forest = RandomForestClassifier(n_estimators=5, random_state=2).fit(Xfake_train, yfake_train)\n","\n","fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n","for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n","    ax.set_title(\"Tree {}\".format(i))\n","    plot_interactive_tree.plot_tree_partition(Xfake_train, yfake_train, tree, ax=ax)\n","\n","plot_2d_separator.plot_2d_separator(forest, Xfake_train, fill=True, ax=axes[-1, -1],\n","                                alpha=.4)\n","axes[-1, -1].set_title(\"Random Forest\")\n","plot_helpers.discrete_scatter(Xfake_train[:, 0], Xfake_train[:, 1], yfake_train)"],"metadata":{"id":"ZdupDB5PuKEW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Enough with fake data. Let's use random forests to predict wages in the NLSY, just as we did for Lasso, Ridge, and Elastic net. Try it on your own! Hint: we want RandomForestRegressor, not RandomForestClassifier. For bonus points, cross-validate random forest's tuning parameters using GridSearchCV."],"metadata":{"id":"iVsT1WDaNAm6"}},{"cell_type":"code","source":["# Try it on your own!\n","\n","# Import the proper package\n","\n","# instantiate your random forest object and fit it\n","\n","# print out training set and test set accuracy"],"metadata":{"id":"Kg6HY2R9ieGU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cheat"],"metadata":{"id":"TknHGNL0ihHX"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","\n","# First without cross-validating\n","rf=RandomForestRegressor(random_state=42).fit(X_train,y_train)\n","print(\"Accuracy on training set: {:.3f}\".format(rf.score(X_train,y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(rf.score(X_test,y_test)))"],"metadata":{"id":"eozvsxHDijQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now with cross-validation\n","# define grid for max_depth\n","param_grid = {'max_depth': [5,10,100]}\n","grid_searchrf = GridSearchCV(RandomForestRegressor(),param_grid,cv=5,return_train_score=True).fit(X_train,y_train)\n","print(\"Best max_depth: \",grid_searchrf.best_estimator_.get_params()['max_depth'])\n","print(\"Accuracy on training set: {:.3f}\".format(grid_searchrf.score(X_train,y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(grid_searchrf.score(X_test,y_test)))"],"metadata":{"id":"1-y1osT_ldU5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How does Random Forest compare with Lasso?"],"metadata":{"id":"91KmD2_Gnhhj"}},{"cell_type":"code","source":[],"metadata":{"id":"UDxfTw76nt8z"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[{"file_id":"15C-wPhQ8lERa6gAsGOFkeLjtH9FIEjFU","timestamp":1696432921698},{"file_id":"1AC4pReS_CGnqgSQqM8yHB5iwzJDK48_K","timestamp":1666389241335}],"collapsed_sections":["XcZtQsXipf9J","qF34B5ittE_x","TknHGNL0ihHX"]}},"nbformat":4,"nbformat_minor":0}