{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "0PjkR6JuKxoY"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import necessary libraries\n",
    "!pip install mglearn\n",
    "!git clone https://github.com/brighamfrandsen/econ484.git\n",
    "%cd econ484/utilities\n",
    "from preamble import *\n",
    "%cd content/econ484/data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brighamfrandsen/econ484/blob/master/examples/neural%20network.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCwPdv7DKxob"
   },
   "source": [
    "### Neural Networks (Deep Learning)\n",
    "#### The Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "QzOd-WulKxoc"
   },
   "outputs": [],
   "source": [
    "display(mglearn.plots.plot_logistic_regression_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "gUsOehShKxoe"
   },
   "outputs": [],
   "source": [
    "display(mglearn.plots.plot_single_hidden_layer_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUx-wH0-Kxog"
   },
   "outputs": [],
   "source": [
    "line = np.linspace(-3, 3, 100)\n",
    "plt.plot(line, np.tanh(line), label=\"tanh\")\n",
    "plt.plot(line, np.maximum(line, 0), label=\"relu\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"relu(x), tanh(x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "heCTg7H0Kxoh"
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_two_hidden_layer_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d34bWCE0Kxoj"
   },
   "source": [
    "#### Tuning Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GH5RebVOKxoj"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                    random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0).fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVl_veinKxol"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='lbfgs', random_state=0, hidden_layer_sizes=[10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1TngtBdKxon"
   },
   "outputs": [],
   "source": [
    "# using two hidden layers, with 10 units each\n",
    "mlp = MLPClassifier(solver='lbfgs', random_state=0,\n",
    "                    hidden_layer_sizes=[10, 10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpWfgdb6Kxop"
   },
   "outputs": [],
   "source": [
    "# using two hidden layers, with 10 units each, now with tanh nonlinearity.\n",
    "mlp = MLPClassifier(solver='lbfgs', activation='tanh',\n",
    "                    random_state=0, hidden_layer_sizes=[10, 100,10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbN2p48fKxor"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "for axx, n_hidden_nodes in zip(axes, [10, 100]):\n",
    "    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\n",
    "        mlp = MLPClassifier(solver='lbfgs', random_state=0,\n",
    "                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\n",
    "                            alpha=alpha)\n",
    "        mlp.fit(X_train, y_train)\n",
    "        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n",
    "        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n",
    "        ax.set_title(\"n_hidden=[{}, {}]\\nalpha={:.4f}\".format(\n",
    "                      n_hidden_nodes, n_hidden_nodes, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-7Ji0-pKxos"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    mlp = MLPClassifier(solver='lbfgs', random_state=i,\n",
    "                        hidden_layer_sizes=[100, 100])\n",
    "    mlp.fit(X_train, y_train)\n",
    "    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n",
    "    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3CS1knMKxou"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "print(\"Cancer data per-feature maxima:\\n{}\".format(cancer.data.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flMgTVSKKxov"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "mlp = MLPClassifier(random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eePUPXPqKxox"
   },
   "outputs": [],
   "source": [
    "# compute the mean value per feature on the training set\n",
    "mean_on_train = X_train.mean(axis=0)\n",
    "# compute the standard deviation of each feature on the training set\n",
    "std_on_train = X_train.std(axis=0)\n",
    "\n",
    "# subtract the mean, and scale by inverse standard deviation\n",
    "# afterward, mean=0 and std=1\n",
    "X_train_scaled = (X_train - mean_on_train) / std_on_train\n",
    "# use THE SAME transformation (using training mean and std) on the test set\n",
    "X_test_scaled = (X_test - mean_on_train) / std_on_train\n",
    "\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWM2cWeeKxoz"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvlevn8CKxo1"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, hidden_layer_sizes=[10], alpha=1, random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31CHLFV5Kxo2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "plt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\n",
    "plt.yticks(range(30), cancer.feature_names)\n",
    "plt.xlabel(\"Columns in weight matrix\")\n",
    "plt.ylabel(\"Input feature\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arpl3aW38zW_"
   },
   "source": [
    "Do cross validation via grid search on alpha to see if you can improve test set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3a0UihHS9QtI"
   },
   "outputs": [],
   "source": [
    "# Don't forget to import the appropriate package!\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# define grid for parameters\n",
    "param_grid = {'hidden_layer_sizes': [[10],[10,10],[100], [100,100]],'alpha': [.01,.1,.5,1],'activation': ['relu','tanh'],'max_iter': [1000]}\n",
    "grid_search = GridSearchCV(MLPClassifier(),param_grid,cv=5,return_train_score=True,n_jobs=-1)\n",
    "best_model=grid_search.fit(X_train_scaled,y_train)\n",
    "print(\"Best hidden_layer_sizes: \",best_model.best_estimator_.get_params()['hidden_layer_sizes'])\n",
    "print(\"Best alpha: \",best_model.best_estimator_.get_params()['alpha'])\n",
    "print(\"bestactivation: \",best_model.best_estimator_.get_params()['activation'])\n",
    "\n",
    "# print out scores\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    best_model.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(best_model.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueSnJ4fJNB0A"
   },
   "source": [
    "Now load another dataset (e.g., mortgages data) and apply neural network classification to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVgrpL4DNRvJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_metadata": {
   "author": "Andreas C. M\\\"ller",
   "title": "Machine Learning with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
