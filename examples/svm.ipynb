{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "fMbmbsABbK4z"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Import necessary libraries\n",
    "!pip install mglearn\n",
    "!git clone https://github.com/brighamfrandsen/econ484.git\n",
    "%cd econ484/utilities\n",
    "from preamble import *\n",
    "%cd content/econ484/data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/brighamfrandsen/econ484/blob/master/examples/svm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1hLcvCibK42"
   },
   "source": [
    "##### Linear models for classification\n",
    "Review:\n",
    "\n",
    "**Maximum Margin Classifiers** use a linear decision boundary and require perfect separation\n",
    "\n",
    "**Support Vector Classifiers** use a linear decision boundary, but allow imperfect separation\n",
    "\n",
    "**Support Vector machines** allow nonlinear decision boundaries and imperfect separation. They use kernel functions to accommodate nonlinearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tdbOS-faXqX"
   },
   "source": [
    "First: let's see how to implement **Support Vector Classifiers**. Note, one probably would always rather use SVMs, but SVCs are nice for visualizing and understanding what is going on. And if the true decision boundary is actually linear, SVCs will do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3weNp26FbK42"
   },
   "outputs": [],
   "source": [
    "# To use SVCs, import its package from sklearn:\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# generate some fake data for visualization:\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "\n",
    "# fit a SVC to the data using the default value for C (look it up on Google to see what it is!)\n",
    "clf = LinearSVC().fit(X, y)\n",
    "\n",
    "# plot the resulting decision boundary and training data points\n",
    "fig, axes = plt.subplots(1, 1, figsize=(3, 3))\n",
    "mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "                                    ax=axes, alpha=.7)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=axes)\n",
    "axes.set_title(clf.__class__.__name__)\n",
    "axes.set_xlabel(\"Feature 0\")\n",
    "axes.set_ylabel(\"Feature 1\")\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj7RhTMDbfxM"
   },
   "source": [
    "Let's play with the regularization parameter, C, to see how it impacts the performance of the SVC. Review: what does C mean in SVCs? (it is the violation \"budget\"). Note that \"C\" as implemented in sklearn is inversely proportional to the degree of regularization: lower values of C means more tolerant of violations, higher values means less tolerant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "ce-7iMmZbK44"
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_svc_regularization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFi7nMoec4k5"
   },
   "source": [
    "Now let's use SVC on some real data, and play with the tuning parameter, C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dby0L-tYbK46"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "\n",
    "lsvc = LinearSVC(C=1,random_state=42).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(lsvc.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lsvc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvd5Q_TZbK48"
   },
   "outputs": [],
   "source": [
    "lsvc100 = LinearSVC(C=100).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(lsvc100.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lsvc100.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kUhLah6bK49"
   },
   "outputs": [],
   "source": [
    "lsvc001 = LinearSVC(C=0.01).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.3f}\".format(lsvc001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.3f}\".format(lsvc001.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsMxW2GCdFXT"
   },
   "source": [
    "On your own, add some code to first standardize the feature matrix (be sure to standardize the test set using the mean and variance adjustment calculated on the training set. Does performance improve after standardizing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43WRl1Tw40yf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPEW9RcObK4_"
   },
   "source": [
    "#### Kernelized Support Vector Machines\n",
    "#### Linear Models and Non-linear Features\n",
    "Let's now allow for non-linear decision boundaries by using SVMs with kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ss7d5Q8kbK5A"
   },
   "outputs": [],
   "source": [
    "# First, generate some fake, nonlinear data\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(centers=4, random_state=8)\n",
    "y = y % 2\n",
    "\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlgBPglCdssC"
   },
   "source": [
    "Let's see linear SVC fail here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d31uIAo5bK5C"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "\n",
    "mglearn.plots.plot_2d_separator(linear_svm, X)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62n-pHSCdzXF"
   },
   "source": [
    "Old-school way to accommodate nonlinear using a linear model: just add squares of the X variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5IT2ybXHwxC"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# assuming mglearn.cm2 is defined or replace with 'plt.cm.coolwarm'\n",
    "\n",
    "# Stack the original features with the square of the second feature\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d', elev=-152, azim=-26)\n",
    "\n",
    "# Create mask to separate classes\n",
    "mask = y == 0\n",
    "\n",
    "# Plot points with y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "\n",
    "# Plot points with y == 1\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "           cmap=mglearn.cm2, s=60, edgecolor='k')\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovkHvIoZeFKV"
   },
   "source": [
    "In 3d space, once we've added the squared term, one can do a reasonable job of separating the blues from the reds with a plane. Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCYNTjVMIfYF"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Fit the model and extract coefficients\n",
    "linear_svm_3d = LinearSVC().fit(X_new, y)\n",
    "coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n",
    "\n",
    "# Define the figure and 3D axes\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d', elev=-152, azim=-26)\n",
    "\n",
    "# Create a grid for the decision surface\n",
    "xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n",
    "yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n",
    "XX, YY = np.meshgrid(xx, yy)\n",
    "\n",
    "# Calculate Z values based on the linear SVM decision boundary\n",
    "if coef[2] != 0:\n",
    "    ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n",
    "    ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n",
    "\n",
    "# Plot data points with classes separated by color and shape\n",
    "mask = y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "           cmap=plt.cm.coolwarm, s=60, edgecolor='k')\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r',\n",
    "           marker='^', cmap=plt.cm.coolwarm, s=60, edgecolor='k')\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cx3pFwhFeROk"
   },
   "source": [
    "A linear plane in 3-dimensions does pretty well! The 3d linear plane corresponds to a nonlinear decision boundary in the original (2d) feature space. Let's plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbOlDQ_rbK5G"
   },
   "outputs": [],
   "source": [
    "ZZ = YY ** 2\n",
    "dec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\n",
    "plt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n",
    "             cmap=mglearn.cm2, alpha=0.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-T1oKSdpbK5I"
   },
   "source": [
    "italicized text#### The Kernel Trick\n",
    "#### Understanding SVMs\n",
    "\n",
    "We can generalize the idea of adding nonlinear terms to the feature set by using kernel functions, which is what SVMs do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSlhCTObbK5I"
   },
   "outputs": [],
   "source": [
    "# To implement a nonlinear SVM, import the SVC package (instead of LinearSVC as we did above)\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# generate some fake data\n",
    "X, y = mglearn.tools.make_handcrafted_dataset()\n",
    "\n",
    "# create and fit our support vector machine, setting tuning parameters:\n",
    "svm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\n",
    "\n",
    "# plot the resulting decision boundary\n",
    "mglearn.plots.plot_2d_separator(svm, X, eps=.5)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "# plot support vectors\n",
    "sv = svm.support_vectors_\n",
    "# class labels of support vectors are given by the sign of the dual coefficients\n",
    "sv_labels = svm.dual_coef_.ravel() > 0\n",
    "mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYwn35SgfIsA"
   },
   "source": [
    "The support vectors are observations exactly on the margin, or that violate the margin (including violating the boundary). They are indicated by thicker shape borders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJxFWffFbK5K"
   },
   "source": [
    "#### Tuning SVM parameters\n",
    "Let's see how different choice of tuning parameters affects the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "id": "-HcQExKkbK5K"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n",
    "\n",
    "for ax, C in zip(axes, [-1, 0, 3]):\n",
    "    for a, gamma in zip(ax, range(-1, 2)):\n",
    "        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n",
    "\n",
    "axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n",
    "                  ncol=4, loc=(.9, 1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phqB0nOTff5B"
   },
   "source": [
    "And now let's try it on some real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQ9BFp_HbK5M",
    "uuid": "19dca39b-9061-4fc6-9aab-f759854ec208"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "svc = SVC(random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ2knWuXfn0w"
   },
   "source": [
    "Let's graph some summary stats on the X variables, because SVMs don't work great when X variables have very different scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55bBsAsIbK5O"
   },
   "outputs": [],
   "source": [
    "plt.boxplot(X_train)\n",
    "plt.yscale(\"symlog\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Feature magnitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEw3ECYGf9D9"
   },
   "source": [
    "Ouch! Those features are all over the place. We could definitely benefit from standardizing our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5ySHmdgbK5P"
   },
   "source": [
    "##### Preprocessing data for SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1QNoY3fbK5Q"
   },
   "outputs": [],
   "source": [
    "# Compute the minimum value per feature on the training set\n",
    "min_on_training = X_train.min(axis=0)\n",
    "# Compute the range of each feature (max - min) on the training set\n",
    "range_on_training = (X_train - min_on_training).max(axis=0)\n",
    "\n",
    "# subtract the min, divide by range\n",
    "# afterward, min=0 and max=1 for each feature\n",
    "X_train_scaled = (X_train - min_on_training) / range_on_training\n",
    "print(\"Minimum for each feature\\n\", X_train_scaled.min(axis=0))\n",
    "print(\"Maximum for each feature\\n\", X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtb01345bK5R"
   },
   "outputs": [],
   "source": [
    "# use THE SAME transformation on the test set,\n",
    "# using min and range of the training set. See Chapter 3 (unsupervised learning) for details.\n",
    "X_test_scaled = (X_test - min_on_training) / range_on_training\n",
    "print(\"Minimum for each feature\\n\", X_test_scaled.min(axis=0))\n",
    "print(\"Maximum for each feature\\n\", X_test_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f47zB3X3gJdy"
   },
   "source": [
    "Now let's re-fit our SVM using our scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmtzO3pzbK5T"
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "        svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXuapoPVgVBR"
   },
   "source": [
    "Big improvement! let's now play around the with tuning parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwI3ZfxubK5V"
   },
   "outputs": [],
   "source": [
    "svc = SVC(C=1,gamma=.8)\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "    svc.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ai_Tho8cnh3"
   },
   "source": [
    "To do on your own: do a formal cross validation to find the optimal values for C and gamma on the scaled cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubXjyfzWggc-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIgAsYe3-149"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1o-xUiruWGWzOd3-QrwVoF5ycqOqoN0t3",
     "timestamp": 1730220221215
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_metadata": {
   "author": "Andreas C. M\\\"ller",
   "title": "Machine Learning with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
